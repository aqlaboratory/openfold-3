================================================================================
OPENFOLD3 CODEBASE OVERVIEW FOR MLX PORTING
================================================================================

PROJECT STATISTICS
- Total Lines of Code: ~72,000
- Number of Files: 250 Python files
- Primary Language: Python with PyTorch

CRITICAL CUDA/GPU COMPONENTS TO REPLACE

1. DeepSpeed4Science (DS4S)
   - File: openfold3/core/model/primitives/attention.py
   - Component: DS4Sci_EvoformerAttention kernel
   - Purpose: Memory-efficient attention for transformer layers
   - Replacement Strategy: Use MLX native optimized attention

2. cuEquivariance (CuEq)
   - File: openfold3/core/model/primitives/attention.py
   - Component: triangle_attention kernel
   - Purpose: Equivariant attention operations
   - Replacement Strategy: MLX native operations with geometric operations

3. Triton Kernels
   - Location: openfold3/core/kernels/triton/
   - Components:
     * triton_softmax.py: Softmax with mask/bias
     * swiglu.py: SwiGLU fused activation
   - Replacement Strategy: MLX native ops (auto-optimized on Apple Silicon)

CORE COMPUTATIONAL MODULES

Attention Layer (672 LOC)
├── Multi-Head Attention (MHA)
│   ├── Q, K, V projections
│   ├── Optional kernel acceleration (DeepSpeed/CuEq)
│   ├── Low-Memory Attention (LMA) option
│   └── Gating mechanism
├── Triangle Attention (pair representations)
├── MSA Row/Column Attention
└── Global Attention

Normalization & Activation
├── LayerNorm (with bfloat16 handling)
├── AdaptiveLayerNorm (conditioning)
├── SwiGLU (with optional Triton fusion)
└── Custom initializations (lecun, he, glorot, gating, final)

Structure Generation
├── Diffusion Module (12.5K LOC)
│   ├── Noise schedule generation
│   ├── Rotation sampling
│   ├── Iterative denoising
│   └── Coordinate generation
├── Prediction Heads
│   ├── pLDDT (local confidence)
│   ├── PAE (aligned error)
│   ├── PDE (distance error)
│   └── Resolution prediction
└── Input Embeddings

Model Architecture
├── MSA Module Stack
│   ├── Row attention
│   ├── Column attention
│   ├── Outer product mean
│   └── Transitions
├── PairFormer Stack
│   ├── Triangle attention
│   ├── Triangle multiplicative updates
│   └── Transitions
└── Diffusion Module (for structure prediction)

GPU MEMORY MANAGEMENT PATTERNS

Pattern 1: CPU-GPU Offloading
- File: openfold3/core/model/latent/evoformer.py
- Implementation: Move intermediate tensors between CPU/GPU during inference
- Issue: Explicit torch.cuda.empty_cache() calls
- MLX Fix: Use MLX memory management (automatic)

Pattern 2: Mixed Precision Training
- File: openfold3/core/utils/precision_utils.py
- Implementation: OF3DeepSpeedPrecision class
- Issue: torch.amp.autocast("cuda") context managers
- MLX Fix: Use MLX dtype system (mx.float32, mx.bfloat16)

Pattern 3: Device Placement
- File: openfold3/entry_points/import_utils.py
- Implementation: torch.set_float32_matmul_precision("high")
- Issue: GPU-specific optimizations
- MLX Fix: Handled by MLX optimizer automatically

DEPENDENCY STACK TO REPLACE

PyTorch Components:
- torch → mlx.core
- torch.nn → mlx.nn / custom MLX modules
- torch.optim → Custom MLX optimizer
- torch.amp → MLX dtype system
- torch.utils.checkpoint → MLX checkpointing
- torch.cuda → MLX device management

Framework Libraries:
- pytorch_lightning → MLX training loop
- deepspeed → MLX native optimization
- triton → MLX native kernels
- cuequivariance → MLX native operations

Keep/Adapt:
- biotite (structure I/O)
- rdkit (chemistry)
- ml-collections (config)
- pdbeccdutils (ligand processing)
- scipy/numpy (numerical ops)

ENTRY POINTS & EXECUTION FLOW

CLI Commands:
1. run_openfold train --runner_yaml config.yml
   - Loads model configuration
   - Creates training runner with DeepSpeed integration
   - Executes training loop

2. run_openfold predict --query_json queries.json
   - Loads model checkpoint
   - Processes input features
   - Performs inference with diffusion sampling
   - Generates structure predictions

3. run_openfold align-msa-server --query_json queries.json
   - MSA computation only (no structure prediction)

Model Forward Pass:
Input → InputEmbedder → MSAModule → PairFormer → DiffusionModule → Confidence Heads → Output

PORTING PRIORITY ROADMAP

Phase 1: FOUNDATION (Required for any output)
- Linear layers (MLX linear ops)
- Attention Q, K, V projections
- Softmax with bias/mask
- Activations (SwiGLU, LayerNorm)
- Basic model structure

Phase 2: COMPUTE EFFICIENCY
- Einsum operations
- Optimized softmax
- Fused operations
- Element-wise operations

Phase 3: MODEL COMPLETENESS
- Full attention mechanisms
- Triangular attention
- Diffusion module
- Loss computation
- All prediction heads

Phase 4: ADVANCED FEATURES
- Checkpointing
- Memory optimization
- Multi-sample generation
- Output serialization

Phase 5: OPTIMIZATION & POLISH
- Performance tuning for Apple Silicon
- Kernel optimization
- Comprehensive testing
- Documentation

ESTIMATED EFFORT

Core Layers: 200-300 LOC per major component
- Attention mechanisms: 3-5 days
- Normalization/Activation: 2-3 days
- Linear/Projections: 1-2 days

Model Architecture: 500-1000 LOC
- MSA Module: 2-3 days
- PairFormer: 2-3 days
- Evoformer: 2-3 days
- Diffusion Module: 3-5 days

Training/Inference Loop: 1000-2000 LOC
- Runners: 2-3 days
- Loss computation: 2-3 days
- Data pipeline: 2-3 days

Total Estimated Effort: 3-4 weeks for core porting
Additional: 1-2 weeks for optimization and testing

KEY CHALLENGES

1. Precision Requirements
   - Biomolecular structures require high numerical precision
   - Need to handle bfloat16 carefully
   - Gradient computation for training

2. Kernel Optimization
   - DeepSpeed/CuEq kernels → MLX native ops
   - Ensure MLX auto-optimizes for Apple Silicon
   - Verify performance parity

3. Mixed Precision Workflows
   - Complex torch.amp.autocast patterns
   - DeepSpeed integration
   - MLX dtype system migration

4. Memory Management
   - CPU-GPU offloading patterns
   - Checkpoint management
   - Large batch processing

5. Testing & Validation
   - Numerical accuracy verification
   - End-to-end inference testing
   - Training convergence validation

================================================================================
