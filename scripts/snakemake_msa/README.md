# Overview

We use the workflow manager [snakemake](https://snakemake.readthedocs.io/en/stable/) to help orchestrate large scale MSA generation. Snakemake distributes jobs efficiently across single node or across a whole cluster. We used this approach to generate MSAs at scale for the PDB and monomer distillation sets. Our pipeline supports both protein alignments and RNA alignments.

# Usage

First, create a conda environment using the `aln_env.yml` file. Next, make sure to download our alignment databases using the `download_of3_databases.py` in this repo. By default, this will download the uniref90, uniprot, mgnify, and template databases for proteins - this requires 330GB of disk space. Additional databases can be downloaded as follows:

```
python download_of3_databases.py --download-bfd # downloads original BFD databases generated by deepmind. Requires 2.3TB of disk space
python download_of3_databases.py --download-cfdb # downloads the ColabFold database that updates BFD, generated by OF3 and the Steinneger lab. Requires 1.5TB disk space 
python download_of3_databases.py --download-rna-dbs # download RNA alignment databases. Requires 27GB disk space
```

Next, modify one of the example configs (ie `example_msa_config_protein.json`) so that the paths to databases and environments match what you just created. A detailed description of the config fields is listed below.: 

```
input_fasta: absolute or relative path to input fasta 
openfold_env: path to openfold conda environment, ie ~/miniforge3/envs/of3
databases: one or more of [uniref90, uniprot, mgnify, cfdb, bfd]
base_database_path: The base directory all alignments dbs are in. Should have the format `{base_directory}/{db}/{db}.fasta` for uniref90, uniprot, mgnify. cfdb/bfd must be downloaded and unpacked into `{base_directory}/{bfd|cfdb}/`
output_directory: output folder to write MSAs to 
jackhmmer_output_format: output format to write jackhmmer MSAs in, one of ["sto", "a3m"]
jackhmmer_threads: number of threads to use for jackhmmer
nhmmer_threads: number of threads to use for nhmmer
hhblits_threads: number of threads to use for hhblits
tmpdir : temporary directory to generate intermediate files
run_template_search: whether or not to run template search with hmmsearch. This requires either: uniref90 to be set as a database, or previously completed unirer90 alignments.
```

You can verify snakemake is configured correctly by running a dryrun with snakemake: 

```
snakemake -np -s MSA_Snakefile --configfile <path/to/config.json>
```

If this runs successfully, you should see no error messages, and list of alignment jobs the pipeline will run. You can then run the main alignment workflow like this. 

```
snakemake -s MSA_Snakefile \
    --cores <available cores> \ 
    --configfile <path/to/config.json>  \
    --nolock  \ 
    --keep-going \
    --latency-wait 120
```

## Best practices

If you need to align both proteins and RNA, its best to make two separate configs and have two separate calls to the pipeline. 

Running multiple databases at once will work just fine, but for generating MSAs at scale ( ie for 100 sequences), we recommend running only a single database at a time - this leads to faster runtimes as the database can be more aggressively cached in memory.

While snakemake has great support for running individual jobs across a cluster, we find that the optimal way to use out alignment pipeline on a typical academic HPC is to submit indpendent snakemake jobs that use a whole node at a time. The main reason for this is that alignments generally work best when the alignment databases are stored on node-local SSD based storage. This typically requires copying data each time a job is run on a node as in most clusters node-local storage is not peristent. Therefore a typical workflow involves first copying alignment DBs to a node, and then running snakemake locally on that node.
